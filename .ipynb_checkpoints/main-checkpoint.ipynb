{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f5255e-bd0a-4470-b2d6-40193636e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW, get_scheduler\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0677d8e0-1d71-422d-9ead-5d1d7be4c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd455fe0-2da6-4ea1-9b57-ba9d1ce6f416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ismai\\\\OneDrive\\\\Masaüstü\\\\Project Adaptation for Code Modeling models'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad0e8f8-ecb3-4c4a-8658-eff4502ef5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/ktorio/ktor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4b7f29-415f-4246-9ddd-5c454fb2b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_kotlin_files(root_dir):\n",
    "    kotlin_files = []\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.kt'):\n",
    "                kotlin_files.append(os.path.join(root, file))\n",
    "    return kotlin_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca2abe6-e1a3-43cd-9d49-9fd9382649b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kotlin_files(kotlin_files):\n",
    "    kotlin_code = []\n",
    "    for file in kotlin_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            code = f.read()\n",
    "            kotlin_code.append(code)\n",
    "    return kotlin_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db4ae2a9-2fb0-4ffb-b5bf-ae8d51733c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root directory of the cloned Ktor repository\n",
    "ktor_repo_dir = cd + '/ktor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "969bddf8-267b-4768-9791-434e5354cb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ismai\\\\OneDrive\\\\Masaüstü\\\\Project Adaptation for Code Modeling models/ktor'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktor_repo_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24d08c4-9460-49d2-94be-8a1d6409c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Kotlin files in the Ktor project directory\n",
    "kotlin_files = find_kotlin_files(ktor_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f19eb3b2-cb3b-4214-bd8e-283d7ceebd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1971"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kotlin_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d0f5b9-374f-405b-a6e9-bcd469d29426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the contents of Kotlin files\n",
    "kotlin_code = read_kotlin_files(kotlin_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22d7b7bf-818e-4f34-bea4-34e1999adbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Kotlin files found: 1971\n",
      "\n",
      "Sample Kotlin code:\n",
      "/*\n",
      " * Copyright 2014-2021 JetBrains s.r.o and contributors. Use of this source code is governed by the Apache 2.0 license.\n",
      " */\n",
      "import org.gradle.api.*\n",
      "import org.gradle.api.tasks.*\n",
      "import org.gradle.kotlin.dsl.*\n",
      "import org.jmailen.gradle.kotlinter.tasks.*\n",
      "\n",
      "fun Project.configureCodestyle() {\n",
      "    apply(plugin = \"org.jmailen.kotlinter\")\n",
      "\n",
      "    kotlinter.apply {\n",
      "        ignoreFailures = true\n",
      "        reporters = arrayOf(\"checkstyle\", \"plain\")\n",
      "    }\n",
      "\n",
      "    val editorconfigFile = rootProject.file(\".editorc\n"
     ]
    }
   ],
   "source": [
    "# Print the number of Kotlin files found\n",
    "print(\"Number of Kotlin files found:\", len(kotlin_files))\n",
    "\n",
    "# Print the first 500 characters of the first Kotlin file as a sample\n",
    "print(\"\\nSample Kotlin code:\")\n",
    "print(kotlin_code[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025647fb-50c5-494f-8357-2f5310a6efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0f4f85-5f0b-4e8a-b108-6cca172f8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model name or path for CodeBERT\n",
    "model_name = \"microsoft/codebert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9db926e-58b7-4959-b347-d389a31a1c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained CodeBERT model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4fd64f4-b5c9-467d-a85a-4e90226adeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer associated with the CodeBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43ddc8a9-817e-4ad9-8a92-ee96e0eed1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input shape: torch.Size([1971, 512])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the Kotlin code\n",
    "tokenized_kotlin_code = tokenizer(kotlin_code, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=512)\n",
    "print(\"Tokenized input shape:\", tokenized_kotlin_code.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff890714-97ad-4fbd-acaf-203de8a1f549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence 1:\n",
      "['<s>', '/*', 'Ċ', 'Ġ*', 'ĠCopyright', 'Ġ2014', '-', '20', '21', 'ĠJet', 'Br', 'ains', 'Ġs', '.', 'r', '.', 'o', 'Ġand', 'Ġcontributors', '.', 'ĠUse', 'Ġof', 'Ġthis', 'Ġsource', 'Ġcode', 'Ġis', 'Ġgoverned', 'Ġby', 'Ġthe', 'ĠApache', 'Ġ2', '.', '0', 'Ġlicense', '.', 'Ċ', 'Ġ*/', 'Ċ', 'import', 'Ġorg', '.', 'grad', 'le', '.', 'api', '.*', 'Ċ', 'import', 'Ġorg', '.', 'grad', 'le', '.', 'api', '.', 't', 'asks', '.*', 'Ċ', 'import', 'Ġorg', '.', 'grad', 'le', '.', 'k', 'ot', 'lin', '.', 'd', 'sl', '.*', 'Ċ', 'import', 'Ġorg', '.', 'j', 'mail', 'en', '.', 'grad', 'le', '.', 'k', 'ot', 'lin', 'ter', '.', 't', 'asks', '.*', 'Ċ', 'Ċ', 'fun', 'ĠProject', '.', 'config', 'ure', 'Cod', 'estyle', '()', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġapply', '(', 'plugin', 'Ġ=', 'Ġ\"', 'org', '.', 'j', 'mail', 'en', '.', 'k', 'ot', 'lin', 'ter', '\")', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġk', 'ot', 'lin', 'ter', '.', 'apply', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġignore', 'Fail', 'ures', 'Ġ=', 'Ġtrue', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġreporters', 'Ġ=', 'Ġarray', 'Of', '(\"', 'check', 'style', '\",', 'Ġ\"', 'plain', '\")', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġval', 'Ġeditor', 'config', 'File', 'Ġ=', 'Ġroot', 'Project', '.', 'file', '(', '\".', 'editor', 'config', '\")', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġtasks', '.', 'with', 'Type', '<', 'L', 'int', 'Task', '>', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġinputs', '.', 'file', '(', 'editor', 'config', 'File', ').', 'with', 'Path', 'S', 'ensitivity', '(', 'Path', 'S', 'ensitivity', '.', 'REL', 'ATIVE', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'Ċ', '}', 'Ċ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Tokenized sequence 2:\n",
      "['<s>', '/*', 'Ċ', 'Ġ*', 'ĠCopyright', 'Ġ2014', '-', '20', '22', 'ĠJet', 'Br', 'ains', 'Ġs', '.', 'r', '.', 'o', 'Ġand', 'Ġcontributors', '.', 'ĠUse', 'Ġof', 'Ġthis', 'Ġsource', 'Ġcode', 'Ġis', 'Ġgoverned', 'Ġby', 'Ġthe', 'ĠApache', 'Ġ2', '.', '0', 'Ġlicense', '.', 'Ċ', 'Ġ*/', 'Ċ', 'import', 'Ġorg', '.', 'grad', 'le', '.', 'api', '.*', 'Ċ', 'import', 'Ġorg', '.', 'grad', 'le', '.', 'k', 'ot', 'lin', '.', 'd', 'sl', '.*', 'Ċ', 'Ċ', 'fun', 'ĠProject', '.', 'config', 'ure', 'Common', '()', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġk', 'ot', 'lin', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġsource', 'S', 'ets', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġcommon', 'Main', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġdependencies', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġapi', '(\"', 'org', '.', 'jet', 'br', 'ains', '.', 'k', 'ot', 'lin', 'x', ':', 'k', 'ot', 'lin', 'x', '-', 'cor', 'out', 'ines', '-', 'core', ':', '${', 'Versions', '.', 'cor', 'out', 'ines', '}', '\")', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'ĊĊ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġcommon', 'Test', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġdependencies', 'Ġ{', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġimplementation', '(', 'k', 'ot', 'lin', '(\"', 'test', '\"))', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ}', 'Ċ', '}', 'Ċ', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Print out tokenized sequences for inspection\n",
    "for i, encoding in enumerate(tokenized_kotlin_code.encodings):\n",
    "    print(f\"Tokenized sequence {i + 1}:\")\n",
    "    print(encoding.tokens)\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4d4ea91-66c1-4842-89e2-299fcef97066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset\n",
    "# kotlin_code_shuffled, _ = train_test_split(kotlin_code, test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2c9237b-27e3-468f-9254-d38189d2d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the shuffled dataset into training, validation, and test sets\n",
    "train_size = 0.6\n",
    "val_test_size = 0.2\n",
    "train_kotlin_code, temp_kotlin_code = train_test_split(kotlin_code, train_size=train_size, random_state=42)\n",
    "val_kotlin_code, test_kotlin_code = train_test_split(temp_kotlin_code, test_size=val_test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "defcc3cc-72d4-40d9-bc70-3a0fc698277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1182\n",
      "Validation set size: 631\n",
      "Test set size: 158\n"
     ]
    }
   ],
   "source": [
    "# Print the sizes of the training, validation, and test sets\n",
    "print(\"Training set size:\", len(train_kotlin_code))\n",
    "print(\"Validation set size:\", len(val_kotlin_code))\n",
    "print(\"Test set size:\", len(test_kotlin_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d27ee0d-f62d-4ffc-ad92-493b6f5d9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KotlinDataset(Dataset):\n",
    "    def __init__(self, tokenized_kotlin_code):\n",
    "        self.tokenized_kotlin_code = tokenized_kotlin_code\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_kotlin_code)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenized_kotlin_code[idx]\n",
    "        input_ids = encoding.ids\n",
    "        attention_mask = encoding.attention_mask\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1334940-0351-4778-bda3-310f8449391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for training, validation, and test\n",
    "train_dataset = KotlinDataset(tokenized_kotlin_code[:len(train_kotlin_code)])\n",
    "val_dataset = KotlinDataset(tokenized_kotlin_code[len(train_kotlin_code):len(train_kotlin_code) + len(val_kotlin_code)])\n",
    "test_dataset = KotlinDataset(tokenized_kotlin_code[len(train_kotlin_code) + len(val_kotlin_code):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "906f3744-a75a-4123-849b-1c429d3a501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e1859e1-729f-45d3-9408-ae7fb0991ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "214f2869-f9ea-4991-b8d7-c86d1ad9f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning configuration\n",
    "learning_rate = 5e-4\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58d10cac-253e-4a12-90bb-fd471d2ac7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismai\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the scheduler\n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),  # 10% of training steps for warmup\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7871c3fb-6a03-46e3-a2f7-e77b33cf648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model in training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Inside the training loop\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch to the appropriate device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # Check if 'loss' attribute exists\n",
    "        if hasattr(outputs, \"loss\") and outputs.loss is not None:\n",
    "            loss = outputs.loss\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Validation loop\n",
    "    val_total_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            val_batch = {k: v.to(device) for k, v in val_batch.items()}\n",
    "            val_outputs = model(**val_batch)\n",
    "            if hasattr(val_outputs, \"loss\") and val_outputs.loss is not None:\n",
    "                val_loss = val_outputs.loss\n",
    "                val_total_loss += val_loss.item()\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = val_total_loss / len(val_dataloader)\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1}: Average training loss {avg_train_loss:.4f}, Average validation loss {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f6910-ce7c-4f3d-b96b-8b08739d1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c8afaf1-65c8-43a3-a43b-f30052b94344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), 'model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bedb0e1d-a4f3-4e13-b556-09dd42454df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new model instance\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15d683d2-6c17-48ed-a69d-4e3b8bdda466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model state dictionary\n",
    "model.load_state_dict(torch.load('model_state.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df8f2f-1d61-43cd-a82c-2c5862220a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
